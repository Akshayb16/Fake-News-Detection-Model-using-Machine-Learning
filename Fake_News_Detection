import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string

data_fake = pd.read_csv('/Users/akshaybhanushali/Desktop/Fake.csv')
data_true = pd.read_csv('/Users/akshaybhanushali/Desktop/True.csv')

data_fake.head()

data_true.head()

data_fake["class"] = 0 #Adding a column called class to the data_fake DataFrame and setting the value to 0 for all rows.
data_true["class"] = 1 #Adding a column called class to the data_true DataFrame and setting the value to 1 for all rows.

data_fake.shape, data_true.shape

data_fake_manual_testing = data_fake.tail(10)  # Save last 10 rows for manual testing

for i in range(23480, 23470, -1):           # Remove those 10 rows from the original dataset
    data_fake.drop(i, axis=0, inplace=True)    #axis = 0 is rows and axis = 1 is column, inplace = True means update the original DataFrame directly
# -1 will tell Python to count backwards, decreasing by 1 each time

data_true_manual_testing = data_true.tail(10) 

for i in range(21416, 21406, -1):           
    data_true.drop(i, axis=0, inplace=True)

data_fake.shape, data_true.shape #now see, 10 rows are deleted from the dataset and it is dataset is updated automatically

data_fake_manual_testing["class"] = 0 
data_true_manual_testing["class"] = 1

data_fake_manual_testing.head()

data_true_manual_testing.head()

data_merge = pd.concat([data_fake, data_true], axis = 0)

data_merge.head(10)

data_merge.columns

data = data_merge.drop(['title', 'subject', 'date'], axis = 1)

data.isnull().sum()

data = data.sample(frac=1)        #this a common trick in pandas used to shuffle the rows of a DataFrame. 
#Can also add random seed if need the same result later.
#frac=1: Means "take 100% of the data" — i.e., don't drop anything, just shuffle all rows.

data.head(10)

data.reset_index(inplace=True)  #The current row indices (which might be shuffled or non-sequential) are replaced with default integers:
data.drop(['index'], axis=1, inplace=True) 

#This resets the row numbers to 0, 1, 2, ...BUT it keeps the old index by moving it into a new column called 'index'
#Hence we drop the column "index"

data.columns

data.head(10)

#def wordopt(text):
    #text = text.lower()                                      # Convert to lowercase
    #text = re.sub(r'\[.*?\]', '', text)                      # Remove text in brackets
    #text = re.sub(r'https?://\S+|www\.\S+', '', text)       # Remove URLs
    #text = re.sub(r'<.*?>+', '', text)                      # Remove HTML tags
    #text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # Remove punctuation
    #text = re.sub(r'\n', '', text)                          # Remove newlines
    #text = re.sub(r'\w*\d\w*', '', text)                    # Remove words with numbers
   # return text

# This meant to clean and normalize text data, usually for natural language processing (NLP) tasks like sentiment analysis or fake news detection.

def wordopt(text):
    text = text.lower()                         # Convert to lowercase
    text = re.sub(r'\[.*?\]', '', text)         # Remove text in brackets
    text = re.sub(r'https?://\S+|www\.\S+', '', text)    # Remove URLs
    text = re.sub(r'<.*?>+', '', text)          # Remove HTML tags
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Don't add extra spaces, Remove punctuation
    text = re.sub(r'\n', ' ', text)             # Remove newlines
    return text.strip()


data['text'] = data['text'].apply(wordopt)
#This replaces the original text column with the cleaned version.

x = data['text']
y = data['class']

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25)

#TF-IDF stands for: Term Frequency - Inverse Document Frequency
#TF – how often a word appears and IDF - reduces importance of common words (like “the”, “is”)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorization = TfidfVectorizer(
    stop_words='english', 
    max_df=0.7, 
    min_df=2,
    ngram_range=(1, 2)   
)

# capture word pairs like "space mission"

# This creates a vectorizer object — like a text-preparation machine.

#At this point, vectorization is ready to: Learn the words in your training data and convert each piece of text into a vector of numbers (word importance)

xv_train = vectorization.fit_transform(x_train) 
#Fit : learns the vocabulary from your training data 
#Transform : transform each sentence/text into a numeric vector

xv_test = vectorization.transform(x_test)


#Raw Text	TF-IDF Vector (example)

#                               [ coding   fun     is    learning  love    machine]
# "I love machine learning" 	 [0.00,   0.00,   0.00,   0.58,    0.58,    0.58]
# "Machine learning is fun" 	[0.00,    0.58,   0.58,   0.45,    0.00,    0.45]
# "I love coding"	            [0.71,    0.00,   0.00,   0.00,    0.71,    0.00]

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(class_weight='balanced', max_iter=1000) # It is used for classification tasks. Basically true(1) or false (0), positive(1) or negative(0)
LR.fit(xv_train, y_train)

pred_lr = LR.predict(xv_test)

LR.score(xv_test, y_test) # It is used to evaluates the performance (accuracy) of your model.

print(classification_report(y_test, pred_lr))

# Checking the detailed summary of my model’s performance across various metrics — not just accuracy.

# It shows:
# Precision	How many predicted positives were actually positive?
# Recall	How many actual positives did the model correctly catch?
# F1-score	The balance between precision and recall (harmonic mean of the two)
# Support	How many actual examples existed in each class?

from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier(class_weight='balanced', 
    max_depth=10,         # limit how deep the tree can go
    min_samples_split=50, # each node must have ≥50 samples to split
    min_samples_leaf=20,  # each leaf must have ≥20 samples
    random_state=42) 

#It mimics human decision-making with yes/no logic.
# It keeps splitting based on features (words in TF-IDF vectors) until it can make a decision.

DT.fit(xv_train, y_train)

pred_dt = DT.predict(xv_test)

DT.score(xv_test, y_test)

print(classification_report(y_test, pred_dt))

from sklearn.ensemble import GradientBoostingClassifier

#Random Forest = A bunch of decision trees trained independently (in parallel), and their votes are averaged.
#Gradient Boosting = A bunch of decision trees trained sequentially, where each tree tries to fix the mistakes of the previous one.

GB = GradientBoostingClassifier(learning_rate=0.1,     # how much each tree “votes” toward the final
    n_estimators=100,      # start with 100 boosting trees
    max_depth=3,           # shallow trees
    min_samples_leaf=20,
    random_state=42)

GB.fit(xv_train, y_train)

pred_gb = GB.predict(xv_test)

GB.score(xv_test, y_test)

print(classification_report(y_test,pred_gb))

from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier(class_weight='balanced')
RF.fit(xv_train, y_train)

pred_rf = RF.predict(xv_test)

RF.score(xv_test, y_test)

print(classification_report(y_test,pred_rf))

def output_lable(n):
    if n == 0: 
        return "Fake News"
    elif n == 1:
        return "Not A Fake News"

def manual_testing(news):
    testing_news = {"text": [news]}
    new_def_test = pd.DataFrame(testing_news)
    new_def_test["text"] = new_def_test["text"].apply(wordopt)
    new_x_test = new_def_test["text"]
    new_xv_test = vectorization.transform(new_x_test)
    pred_LR = LR.predict(new_xv_test)
    #pred_DT = DT.predict(new_xv_test)
    #pred_GB = GB.predict(new_xv_test)
    pred_RF = RF.predict(new_xv_test)
    
    return print("\n\nLR Prediction: {} \nRF Prediction: {}" .format(output_lable(pred_LR[0]),output_lable(pred_RF[0])))
                                                                                                              #output_lable(pred_GB[0]),
                                                                                                              #output_lable(pred_RF[0])))

news = str(input())
manual_testing(news)
